# -*- coding: utf-8 -*-
"""project rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MNJOijTgvWbyyYOE2V_1_vkhkxqFnWB2
"""

nltk.download('punkt_tab')

import os
import re
import glob
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
nltk.download('punkt')

# Path to extracted text files
text_folder = '/content/Rag Project/extracted_texts' # Update with actual path
text_files = glob.glob(os.path.join(text_folder, "*.txt"))

def chunk_text(text, chunk_size=1000):
    words = word_tokenize(text)
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

# Read and chunk text files
documents = []
chunked_documents = []
for file in text_files:
    with open(file, "r", encoding="utf-8") as f:
        text = f.read()
        documents.append(text)
        chunks = chunk_text(text)
        chunked_documents.extend(chunks)

# Basic Stats After Chunking
doc_lengths = [len(word_tokenize(doc)) for doc in chunked_documents]

print("Total number of chunks:", len(chunked_documents))
print("Average chunk length (words):", sum(doc_lengths) / len(chunked_documents))
print("Max chunk length (words):", max(doc_lengths))
print("Min chunk length (words):", min(doc_lengths))

from google.colab import drive
drive.mount('/content/drive')

import nltk
nltk.download('punkt_tab')

folder_path = '/content/Rag Project/extracted_texts'

def load_text_files(folder_path):
    text_data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                text_data.append(file.read())
    return text_data

texts = load_text_files(folder_path)

# Path to extracted text files
text_folder = '/content/Rag Project/extracted_texts'
text_files = glob.glob(os.path.join(text_folder, "*.txt"))

# Clean headers
def clean_header(text):
    text = re.sub(r'DIRECTIVE.*?EU.*?the European Parliament.*?Council', '', text)  # Adjust regex for your specific header patterns
    text = re.sub(r'Official Journal of the European Union.*?(\d{1,2}\.\d{1,2}\.\d{4})', '', text)
    return text

# Clean and normalize text (remove extra spaces and non-alphanumeric characters)
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove excessive spaces
    text = re.sub(r'[^\w\s.,!?()]', '', text)  # Keep basic punctuation including parentheses for footnotes
    return text.strip()

# Retain section numbers (optional, if needed for context)
def preserve_numbered_sections(text):
    text = re.sub(r'(\d+\.\s)', r' \1', text)  # Example: "1. Some section" => " 1. Some section"
    return text

# Preprocess the text files
def preprocess_text(text):
    text = clean_header(text)  # Clean headers
    cleaned_text = clean_text(text)  # Clean text (whitespace, punctuation)
    cleaned_text = preserve_numbered_sections(cleaned_text)  # Optional: Retain section numbering
    return cleaned_text

# Read, preprocess, and chunk text files
documents = []
chunked_documents = []
for file in text_files:
    with open(file, "r", encoding="utf-8") as f:
        text = f.read()
        # Preprocess the text (remove headers, clean text, but keep footnotes)
        cleaned_text = preprocess_text(text)
        documents.append(cleaned_text)

        # Chunk the cleaned text into smaller pieces
        chunks = chunk_text(cleaned_text)  # Assuming chunk_text is defined as you did earlier
        chunked_documents.extend(chunks)

# Basic Stats After Chunking
doc_lengths = [len(word_tokenize(doc)) for doc in chunked_documents]

print("Total number of chunks:", len(chunked_documents))
print("Average chunk length (words):", sum(doc_lengths) / len(chunked_documents))
print("Max chunk length (words):", max(doc_lengths))
print("Min chunk length (words):", min(doc_lengths))

#pip install sentence-transformers

#pip install -U langchain-community

from sentence_transformers import SentenceTransformer
import numpy as np

# Load pre-trained embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for the chunks
chunk_embeddings = model.encode(chunked_documents, show_progress_bar=True)

# Example: Checking the shape of embeddings
print("Shape of embeddings:", np.shape(chunk_embeddings))

#pip install chromadb

import chromadb

# Initialize ChromaDB
client = chromadb.Client()

# Try to get the existing collection
collection = client.get_collection(name="financial_laws_collection")

# Store the embeddings in the collection with metadata (e.g., chunk text)
for i, embedding in enumerate(chunk_embeddings):
    collection.add(
        documents=[chunked_documents[i]],  # The original chunk text
        embeddings=[embedding],  # The embedding for the chunk
        metadatas=[{"source": text_files[i % len(text_files)]}],  # Optionally include file metadata (e.g., source document)
        ids=[str(i)]  # Unique ID for each chunk
    )

# Example: Querying the collection (retrieve top 5 most similar chunks)
query = "What is the recovery framework for financial institutions?"
query_embedding = model.encode([query])[0]  # Embed the query

# Perform a similarity search
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5
)

# Print the results
for i, result in enumerate(results['documents']):
    print(f"Result {i+1}:\n{result}\n")

# Ensure we are accessing the first document correctly
cleaned_result = results['documents'][0][0].strip()  # Extract inner string and strip whitespace
print(f"Cleaned result:\n{cleaned_result}\n")

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

# Initialize embeddings
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Connect to ChromaDB
vectorstore = Chroma(
    persist_directory="./chroma_db",  # Specify the directory where ChromaDB is stored
    embedding_function=embedding_model
)

# Convert stored texts into LangChain Documents
docs = [Document(page_content=text, metadata={"source": text_files[i % len(text_files)]})
        for i, text in enumerate(chunked_documents)]

# Add documents to vector store
vectorstore.add_documents(docs)

# Use LangChain's retriever to fetch relevant documents
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})  # Retrieve top 5 relevant chunks

# Test retrieval with a sample query
query = "What is the recovery framework for financial institutions?"
retrieved_docs = retriever.get_relevant_documents(query)

# Print results
for i, doc in enumerate(retrieved_docs):
    print(f"Result {i+1}:\n{doc.page_content}\n")

# Connecting to a Large Language Model (LLM)

#pip install openai langchain

#pip install transformers torch

#pip install openai

import os
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Set OpenAI API Key
os.environ["OPENAI_API_KEY"] = "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"

# Initialize GPT-4
llm = ChatOpenAI(model_name="gpt-4", temperature=0)

# Define a prompt template
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="You are an expert in financial laws. Based on the following information: {context}, answer the question: {question}"
)

# Create Retrieval-Augmented Generation (RAG) Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)

# Query the system
query = "What is the recovery framework for financial institutions?"
response = qa_chain.run(query)

print(f"Answer:\n{response}")

query = "What are the key requirements for credit institutions under Directive 2014/59/EU?"
response = qa_chain.run(query)
print(response)

query = "What is the recovery framework for financial institutions?"
response = qa_chain.run(query)

# Add citation from retrieved docs
sources = [doc.metadata["source"] for doc in retrieved_docs]
formatted_response = f"{response}\n\nSources: {', '.join(set(sources))}"

print(formatted_response)

"""## Build an API (FastAPI)"""

#BELOW CELLS TO BE IGNORED

#pip install fastapi uvicorn nest-asyncio pyngrok

#!pip install pyngrok
#!ngrok authtoken 2shFic7VAfZNVlipzgNQwNcvMub_37iRFjnCGQpmrGScFyb5Y

import threading
import time
from fastapi import FastAPI
from pydantic import BaseModel
import chromadb
import torch
from transformers import AutoModel, AutoTokenizer
import nest_asyncio
import uvicorn
from pyngrok import ngrok

# Initialize FastAPI
app = FastAPI()

# Apply nest_asyncio to avoid event loop issues
nest_asyncio.apply()

# Load embedding model
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name)

# Connect to ChromaDB
client = chromadb.PersistentClient(path="/content/chroma_db")
collection = client.get_collection(name="langchain")  # Ensure this collection exists

# Pydantic model for requests
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5

# Function to generate embeddings
def generate_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = embedding_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().tolist()

# FastAPI endpoint
@app.post("/query")
def query_rag(request: QueryRequest):
    query_embedding = generate_embedding(request.query)
    results = collection.query(query_embeddings=[query_embedding], n_results=request.top_k)
    return {"results": results["documents"]}

# Start ngrok tunnel for external access
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")

# Function to run the API in the background
def run_api():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Run FastAPI in a separate thread
thread = threading.Thread(target=run_api, daemon=True)
thread.start()

# Keep the Colab cell alive
while True:
    time.sleep(10)

import requests

url = "https://5936-34-75-106-8.ngrok-free.app/query"  # Replace with your ngrok URL
data = {"query": "What is the recovery framework for financial institutions?", "top_k": 3}

response = requests.post(url, json=data)
print(response.json())  # Should return the most relevant documents

import requests

api_url = "YOUR_NGROK_URL/query"  # Replace with your actual ngrok URL

payload = {
    "query": "What is the recovery framework for financial institutions?",
    "top_k": 3
}

response = requests.post(api_url, json=payload)
print(response.json())









